\documentclass[
aps,%
12pt,%
final,%
notitlepage,%
oneside,%
onecolumn,%
nobibnotes,%
nofootinbib,% 
superscriptaddress,%
noshowpacs,%
amsmath,%
amssymb,%
centertags]%
{revtex4-2}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}

\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{booktabs}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{definition}{Определение}
\newtheorem{assumption}{Предположение}
\newtheorem{property}{Свойство}
\newtheorem{corollary}{Следствие}
\newtheorem{remark}{Замечание}
\newtheorem{hypothesis}{Гипотеза}

\begin{document}

\title{ЛАНДШАФТНАЯ МЕРА МОДЕЛЕЙ ГЛУБОКОГО ОБУЧЕНИЯ}

\author{\firstname{А.~В.}~\surname{Грабовой}}
\email{grabovoy@ipu.ru}
\affiliation{%
    Институт проблем управления им. В. А. Трапезникова Российской академии наук (ИПУ РАН)
}%

\begin{abstract}
В работе разрабатывается единый теоретический формализм для формализации соотношения между сложностью модели и сложностью данных в контексте обучения глубоких нейронных сетей. Предлагаемый подход основан на введении мер сложности в рамках теории мер, что позволяет формализовать критерий обучаемости модели на выборке и установить строгие теоретические связи между свойствами архитектуры модели и характеристиками данных. Ключевая идея заключается в установлении формального соотношения между мерой сложности модели и мерой сложности данных, определяемого через условие обучаемости. Основным инструментом анализа служит изучение изменения функции потерь при непрерывном изменении выборки, что обеспечивает установление строгих теоретических связей между спектральными свойствами матриц Гессе и обобщающей способностью моделей. В работе вводятся формальные определения меры сложности выборки и меры сложности модели, а также устанавливается критерий обучаемости модели на выборке. Предлагаемый формализм обеспечивает вычислительно осуществимые методы оценки сложности, основанные на аналитических оценках спектральных свойств матриц Гессе.
\end{abstract}

\maketitle

\section{Введение}

Существующие подходы к анализу сложности моделей глубокого обучения характеризуются существенными ограничениями. Классические меры сложности не учитывают специфику перепараметризованных нейронных сетей. Сложность модели и данных рассматриваются изолированно без формальных критериев их соответствия. Анализ матриц Гессе остается в основном эмпирическим и не обеспечивает строгих теоретических связей для конкретных архитектур. В результате отсутствует единый формальный аппарат, способный связать сложность модели и данных в рамках строгой теоретической основы, применимой к широкому классу архитектур нейронных сетей.

Значительное развитие получило направление анализа ландшафта функции потерь и спектральных свойств матриц Гессе как меры сложности нейросетевых моделей. Эмпирические исследования показали, что матрицы Гессе перепараметризованных нейронных сетей обладают характерной структурой: большинство собственных значений сосредоточены около нуля, а небольшое число больших собственных значений определяет кривизну ландшафта. Минимумы с малым спектральным радиусом Гессиана коррелируют с лучшей обобщающей способностью. Однако большинство результатов остаются эмпирическими и зависят от режима оптимизации, архитектуры и данных.

Для полносвязных нейронных сетей получены строгие теоретические оценки спектральной нормы матрицы Гессе~\cite{grabovoi2024unraveling748584228}, показавшие, что норма гессиана ограничена выражением, которое устанавливает экспоненциальную зависимость от глубины сети и полиномиальную зависимость от ширины слоев. Асимптотическая оценка демонстрирует пропорциональность, зависящую от размера скрытого слоя и константы, ограничивающей параметры и данные.

Для сверточных нейронных сетей получены теоретические оценки нормы гессиана через структурные параметры архитектуры~\cite{grabovoi2024convnets743111032}. Для одномерных сверточных сетей установлена оценка, демонстрирующая мультипликативную зависимость от глубины и полиномиально-экспоненциальную зависимость от числа каналов, размера ядра и длины последовательности. Для двумерных сверточных сетей получены аналогичные оценки, учитывающие двумерную природу данных.

Ключевым результатом в анализе матриц Гессе является их декомпозиция на G-компоненту и H-компоненту~\cite{sagun2018empiricalanalysishessianoverparametrized,skorski2019chainruleshessianhigher}. G-компонента отражает кривизну функции потерь относительно выходов сети, тогда как H-компонента описывает кривизну самой нейронной сети. Эта декомпозиция позволяет анализировать вклад различных факторов в общую сложность оптимизационного ландшафта и связывает свойства Гессиана с обобщающей способностью моделей. Эмпирические исследования показали, что минимумы с малым спектральным радиусом Гессиана коррелируют с лучшей обобщающей способностью~\cite{keskar2016large,dinh2017sharp}, причем ``плоские'' минимумы могут обобщаться лучше, чем ``острые'', хотя эта связь не является универсальной и зависит от архитектуры и данных.

Визуализация ландшафта функции потерь~\cite{li2018visualizing} позволила эмпирически исследовать геометрию оптимизационного пространства. Методы визуализации через фильтрацию случайных направлений и анализ одномерных сечений показали, что модели, достигшие высокого качества обобщения, локализуются в областях с плавным ландшафтом, тогда как модели с низким качеством обобщения характеризуются минимумами с высокой кривизной. Структурный анализ матриц Гессе на больших масштабах~\cite{papyan2019spectrumdeepnethessiansscale,pmlr-v97-ghorbani19b} исследовал динамику спектра Гессиана в процессе обучения и его зависимость от размера выборки. Было установлено, что распределение собственных значений Гессиана следует степенным законам, причем ``хвост'' распределения содержит информацию о критических направлениях в пространстве параметров.

Для сверточных нейронных сетей были получены теоретические оценки структуры матриц Гессе~\cite{singh2023hessianperspectivenatureconvolutional}, показавшие, что сверточная структура приводит к блочно-циркулянтным свойствам Гессиана. Для трансформеров был проведен теоретический анализ Гессиана~\cite{ormaniec2024attentionhessian}, связавший его структуру с механизмами внимания и показавший, как архитектурные особенности отражаются в спектральных свойствах. Вычислительные методы для анализа Гессиана включают быстрые алгоритмы умножения на Гессиан~\cite{pearlmutter1994fast} и библиотеки~\cite{yao2020pyhessian}, позволяющие эффективно вычислять собственные значения и след матрицы Гессе для больших моделей, что сделало возможным эмпирический анализ ландшафта функции потерь для современных архитектур.

Теоретические результаты о структуре ландшафта исследовали спектр матрицы Фишера~\cite{pennington2017spectrum} и показали возникновение спектральной универсальности в глубоких сетях~\cite{pennington2018emergence}. Было доказано, что градиентный спуск находит глобальные минимумы для достаточно широких сетей~\cite{gurari2018gradient}, что связано с уменьшением кривизны ландшафта в пределе бесконечной ширины. Методы регуляризации, основанные на анализе Гессиана, включают подходы~\cite{chaudhari2016entropy}, которые смещают траекторию оптимизации в области с меньшей кривизной ландшафта и используют информацию о кривизне для адаптивной настройки шага обучения. Исследования крупномасштабной структуры ландшафта~\cite{fort2019large,fort2019energy} показали наличие иерархии минимумов и связь между геометрией ландшафта и обобщающей способностью.

Эмпирические законы масштабирования~\cite{kaplan2020ScalingLaws} установили, что функция потерь языковых моделей подчиняется степенным законам относительно числа параметров, объема обучающих данных и вычислительного бюджета. Эти зависимости показывают, что для достижения заданного уровня качества необходимо синхронно масштабировать как модель, так и данные, причем влияние данных оказывается несколько сильнее. Было формализовано оптимальное соотношение между числом параметров и объемом данных~\cite{hoffmann2022Chinchila}, демонстрирующее критическую важность баланса между сложностью модели и объемом данных.

Несмотря на значительный прогресс в получении теоретических оценок спектральных норм матриц Гессе для различных архитектур и эмпирических закономерностей, большинство результатов остаются эмпирическими и зависят от режима оптимизации, архитектуры и данных. Строгие теоретические связи между спектральными свойствами Гессиана и обобщающей способностью установлены лишь для ограниченных классов моделей. Отсутствует единый формализм, связывающий эти оценки с мерой сложности данных и устанавливающий формальные критерии обучаемости модели на выборке. Для моделей с миллионами и миллиардами параметров прямое вычисление и анализ матриц Гессе становится непрактичным из-за квадратичной сложности по памяти и вычислениям, что мотивирует разработку аналитических методов оценки сложности.

Для преодоления указанных ограничений в настоящей работе разрабатывается единый теоретический формализм, устанавливающий формальное соотношение между сложностью модели и сложностью данных. Предлагаемый подход основан на введении мер сложности в рамках теории мер, что позволяет формализовать критерий обучаемости модели на выборке и установить строгие теоретические связи между свойствами архитектуры модели и характеристиками данных.

Основная цель настоящей работы заключается в установлении формального соотношения между мерой сложности модели и мерой сложности данных, определяемого через условие обучаемости, согласно которому сложность модели не должна превышать сложности данных. В работе также получены частные случаи, допускающие более детальный практический и теоретический анализ.

Основным инструментом анализа в предложенном подходе служит изучение изменения функции потерь при непрерывном изменении выборки. Данный подход обеспечивает установление строгих теоретических связей между спектральными свойствами матриц Гессе и обобщающей способностью моделей. Предлагаемый формализм обеспечивает вычислительно осуществимые методы оценки сложности, основанные на аналитических оценках спектральных свойств матриц Гессе.

Структура работы организована следующим образом. В разделе~\ref{chapter:complexity:models-data} вводятся формальные определения меры сложности выборки и меры сложности модели в рамках теории мер, а также устанавливается критерий обучаемости модели на выборке. В разделе~\ref{chapter:complexity:loss} разрабатывается ландшафтная мера сложности модели на основе анализа сходимости функции потерь.

\section{Мера сложности моделей}\label{chapter:complexity:models-data}

В современной теории глубокого обучения фундаментальной проблемой является установление соответствия между сложностью модели и характеристиками данных. Существующие подходы рассматривают сложность модели и сложность данных изолированно, не устанавливая формальных критериев их соответствия. В настоящем разделе производится формализация мер сложности выборки и модели в рамках теории мер, что позволяет установить строгий критерий обучаемости, связывающий свойства модели с характеристиками данных.

\begin{definition}[Генеральная совокупность данных]\label{chapter:complex:def-gamma}
    Генеральной совокупностью данных~$\Gamma$ назовем произвольное множество объектов, которые исследуются в рамках той или иной задачи. В общем случае нет никаких ограничений на счетность множества генеральной совокупности.
\end{definition}

Определение~\ref{chapter:complex:def-gamma} позволяет работать как с однородными, так и с многородными генеральными совокупностями.

\begin{definition}[Однородная и многородная генеральная совокупность]\label{chapter:complex:def-gamma-modality}
    Генеральную совокупность~$\Gamma$ назовем однородной, если все объекты генеральной совокупности порождаются из одного распределения. В противном случае генеральную совокупность назовем~$k$-родной, где~$k$ является числом распределений, на основе которых была сгенерирована генеральная совокупность.
\end{definition}

В определении~\ref{chapter:complex:def-gamma-modality} примером двуродной генеральной совокупности служит выборка, состоящая из текстов и изображений в качестве объектов исследования. Современные большие языковые модели, которые одновременно обрабатывают тексты и изображения и называются многомодальными моделями, представляют собой пример работы с многородовыми генеральными совокупностями.

Пусть задана генеральная совокупность данных~$\Gamma$. Множество всех подмножеств объектов, образующих кольцо выборок, обозначим как:
\[
    \mathfrak{D} = \{D_\Gamma^i\}, \quad D_\Gamma^i \subset \Gamma.
\]

\begin{definition}[Мера сложности выборки]\label{chapter:complex:def-data-complexity}
    Мерой сложности выборки назовем отображение~$\mu_D,$ такое, что: 
    \[
        \mu_D(D_i) : \mathfrak{D}_\Gamma \to \mathbb{R}_+,
    \]
    удовлетворяющее свойству:
    \begin{align}
        \mu_D(D_i\cup D_j) \leq \mu_D(D_i)+\mu_D(D_j),
    \end{align}
    где равенство достигается при условии~$D_i\cap D_j=\emptyset.$
\end{definition}

Определение~\ref{chapter:complex:def-data-complexity} является классическим определением из теории меры, удовлетворяющим свойству конечной аддитивности. Конечной аддитивности достаточно, так как в рамках настоящей работы рассматривается конечное число объектов при обучении моделей глубокого обучения.
Сравнение выборок предполагается только из одной генеральной совокупности, однако этим никак не ограничивается сама генеральная совокупность, и в определении допускаются мультимодальные генеральные совокупности.

\begin{lemma}[Монотонность меры сложности выборки]\label{chapter:complex:lemma-monotonicity}
    Мера сложности выборки $\mu_D$, определенная в определении~\ref{chapter:complex:def-data-complexity}, обладает свойством монотонности: для любых выборок $D_1, D_2 \in \mathfrak{D}$ таких, что $D_1 \subseteq D_2$, выполняется неравенство
    \[
        \mu_D(D_1) \leq \mu_D(D_2).
    \]
\end{lemma}
\begin{proof}
    Пусть $D_1 \subseteq D_2$. Представим $D_2$ в виде объединения $D_2 = D_1 \cup (D_2 \setminus D_1)$, где $D_1 \cap (D_2 \setminus D_1) = \emptyset$. По свойству субаддитивности меры сложности выборки из определения~\ref{chapter:complex:def-data-complexity} имеем:
    \[
        \mu_D(D_2) = \mu_D(D_1 \cup (D_2 \setminus D_1)) \leq \mu_D(D_1) + \mu_D(D_2 \setminus D_1).
    \]
    Учитывая, что $D_1 \cap (D_2 \setminus D_1) = \emptyset$, по определению~\ref{chapter:complex:def-data-complexity} равенство в свойстве субаддитивности достигается:
    \[
        \mu_D(D_2) = \mu_D(D_1 \cup (D_2 \setminus D_1)) = \mu_D(D_1) + \mu_D(D_2 \setminus D_1).
    \]
    Поскольку мера сложности выборки принимает неотрицательные значения ($\mu_D : \mathfrak{D}_\Gamma \to \mathbb{R}_+$), имеем $\mu_D(D_2 \setminus D_1) \geq 0$. Следовательно,
    \[
        \mu_D(D_2) = \mu_D(D_1) + \mu_D(D_2 \setminus D_1) \geq \mu_D(D_1) + 0 = \mu_D(D_1),
    \]
    что и требовалось доказать.
\end{proof}

\begin{remark}
    Лемма~\ref{chapter:complex:lemma-monotonicity} устанавливает монотонность меры сложности выборки как следствие субаддитивности. Данное свойство является общим свойством мер конечных множеств в теории меры. Однако в случае бесконечных выборок монотонность может не выполняться автоматически и требует более строгих формулировок в определении меры выборок, таких как счетная аддитивность или непрерывность меры снизу. В рамках настоящей работы рассматриваются только конечные выборки, что соответствует практическим задачам обучения моделей глубокого обучения.
\end{remark}


Пусть задано множество параметрических аппроксимирующих моделей
\[
    \mathfrak{F} = \left\{f_i\right\},
\]
где каждое~$f_i$ является некоторым множеством параметрических функций. В определении~\ref{chapter:complex:def-model} вводится характеристика параметрического семейства функций~$f$, которые в дальнейшем рассматриваются в качестве моделей глубокого обучения.
\begin{definition}[Мера сложности модели]\label{chapter:complex:def-model}
    Мерой сложности модели~$f$ назовем отображение~$\mu_f(f_i)$:
    \[
        \mu_f(f_i) : \mathfrak{F} \to \mathbb{R}_+.
    \]
\end{definition}

Определение меры сложности модели~$f$ не является определением меры в общем случае, так как множество~$\mathfrak{F}$ не является кольцом. Поэтому данная мера представляет собой некоторое отображение, которое является характеристикой сложности.
Примером меры сложности модели, удовлетворяющей определению~\ref{chapter:complex:def-model}, является число параметров модели.

После введения определений меры сложности как для выборки, так и для модели, вводится определение обучаемости модели на выборке, которое сформулировано в определении~\ref{chapter:complex:def-model-bound-data}.

\begin{definition}[Обучаемость модели на выборке]\label{chapter:complex:def-model-bound-data}
    Назовем модель $f\in\mathfrak{F}$ \textit{обучаемой} на выборке $D\in\mathfrak{D},$ если 
    \[
        \mu_f(f)\leq \mu_D(D).
    \]
\end{definition}

В определении~\ref{chapter:complex:def-model-bound-data} не вводится никакого ограничения на качество аппроксимации модели после обучения. Более подробно это будет определено для частных случаев мер в следующих разделах. 

Определение~\ref{chapter:complex:def-model-bound-data} допускает следующую эмпирическую интерпретацию: сложность модели не должна превышать сложности данных, на которых она обучается. В противном случае возникает проблема переобучения, когда модель запоминает шум в данных вместо выявления значимых закономерностей. Указанный критерий формализует представление о балансе между выразительной способностью модели и информационной емкостью данных.

\begin{theorem}[Необходимое условие дообучаемости модели]\label{chapter:complex:theorem-finetunning}
    Если для исходной выборки~$D\in\mathfrak{D}$ выполняется условие~$\mu_f(f) \leq \mu_D(D)$, тогда для новой выборки~$D'\in\mathfrak{D}$ необходимое условие дообучаемости модели на объединенной выборке $D \cup D'$ имеет вид:
    \[
        \mu_f(f) - \mu_D(D) \leq \mu_D(D').
    \]
\end{theorem}
\begin{proof}
Доказательство основано на свойствах мер сложности и условии обучаемости модели. 

По определению обучаемости модели на выборке $D$ (определение~\ref{chapter:complex:def-model-bound-data}) имеем:
\[
    \mu_f(f) \leq \mu_D(D).
\]
При добавлении новых данных $D'$ к исходной выборке $D$ по лемме~\ref{chapter:complex:lemma-monotonicity} о монотонности меры сложности выборки получаем:
\[
    \mu_D(D) \leq \mu_D(D\cup D').
\]
Из свойства субаддитивности меры сложности выборки (определение~\ref{chapter:complex:def-data-complexity}) получаем:
\[
    \mu_D(D\cup D') \leq \mu_D(D)+\mu_D(D'),
\]
где равенство достигается при условии $D \cap D' = \emptyset$. Объединяя эти три неравенства, получаем цепочку:
\[
    \mu_f(f) \leq \mu_D(D) \leq \mu_D(D\cup D') \leq \mu_D(D)+\mu_D(D'),
\]
откуда, перенося $\mu_D(D)$ в левую часть, получаем окончательное неравенство:
\[
    \mu_f(f) - \mu_D(D) \leq \mu_D(D').
\]
Для того чтобы модель была обучаемой на объединенной выборке $D \cup D'$, необходимо выполнение условия $\mu_f(f) \leq \mu_D(D \cup D')$. Полученное неравенство $\mu_f(f) - \mu_D(D) \leq \mu_D(D')$ является необходимым для выполнения этого условия, что завершает доказательство.
\end{proof}

Полученное неравенство демонстрирует, что оставшаяся емкость модели, определяемая как разность $\mu_f(f) - \mu_D(D)$ между сложностью модели и сложностью исходных данных, не превосходит сложности новых данных~$D'$. Указанное условие является необходимым для успешного дообучения модели на новых данных: если оставшаяся емкость превышает сложность новых данных, модель не сможет эффективно адаптироваться к расширенной выборке. Достаточность данного условия зависит от конкретного выбора мер сложности и может требовать дополнительных предположений о структуре данных и модели.

Таким образом, введение формальных мер сложности моделей и данных создает теоретическую основу для решения практических задач проектирования архитектур нейронных сетей, планирования экспериментов и оптимизации процессов обучения.

\section{Ландшафтная мера моделей глубокого обучения}\label{chapter:complexity:loss}

Рассмотрим выборку из простой генеральной совокупности~$\Gamma_C$:
\begin{equation}
    D = \left\{ (\mathbf{x}_i, \mathbf{y}_i) \right\}, \quad i = 1, \ldots, m, \quad \mathbf{x} \in \mathcal{X}, \ \mathbf{y} \in \mathcal{Y}, \quad D\subset \Gamma_C.
\end{equation}

Рассмотрим некоторое параметрическое отображение~$f_{\boldsymbol{\theta}}: \mathcal{X} \to \mathcal{Y},$ которое аппроксимирует условное распределение целевой переменной для заданного признакового описания объекта~$p(\mathbf{y}|\mathbf{x}).$ Параметры~$\boldsymbol{\theta}$ функции~$f_{\boldsymbol{\theta}}$ принадлежат пространству~$\mathbb{R}^{P},$ где~$P$ описывает число параметров отображения~$f_{\boldsymbol{\theta}}$.

Пусть, для выбора оптимального вектора параметров~$\hat{\boldsymbol{\theta}}$ используется подход минимизации эмпирического риска:
\begin{equation}
    \hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} \mathcal{L}_m(\boldsymbol{\theta}),
\end{equation}
где функция эмпирического риска для выборки размера~$|D|=m$ задается в следующем виде: 
\begin{equation}
    \mathcal{L}_m(\boldsymbol{\theta}) = \frac{1}{m} \sum\limits_{i=1}^{m} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), \mathbf{y}_i) \approx \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p(\mathbf{x}, \mathbf{y})} \left[ \ell(f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y}) \right],
\end{equation}
где функция~$\ell\left(\mathbf{z}, \mathbf{y}\right)$ описывает ошибку на одном объекте. Далее в качестве функции~$\ell$ будут рассматриваться либо кросс-энтропийная функция потерь, либо средняя квадратическая ошибка, в зависимости от рассматриваемой задачи и архитектуры модели.

Функция эмпирического риска~$\mathcal{L}_m(\boldsymbol{\theta})$ задает некоторую поверхность в пространстве параметров размерности~$P$. Изучение изменения этой поверхности при добавлении новых объектов данных позволяет количественно оценить влияние объема выборки на оптимизационный ландшафт.

Изменение значения функции потерь при добавлении одного объекта вычисляется следующим образом:
\begin{align}\label{chapter:complex:equation-difference}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1}\sum_{i=1}^{k+1}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) - \frac{1}{k}\sum_{i=1}^k\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \\
    &= \frac{1}{k+1}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1})-\sum_{i=1}^{k}\frac{1}{k(k+1)}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \\
    &= \frac{1}{k+1} \left(\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_{k}(\boldsymbol{\theta})\right).
\end{align}

Дальнейшее исследование ландшафта нацелено на изучение данной разницы. Особый интерес представляют предельные свойства при стремлении размера выборки к бесконечности. Для дальнейших оценок данной разности вводится предположение~\ref{chapter:complex:assumption-local-optima-not-change}, которое подтверждается на практике, однако является достаточно сильным, что упрощает дальнейшие выкладки.

\begin{assumption}\label{chapter:complex:assumption-local-optima-not-change}
    Пусть $\boldsymbol{\theta}^*$ является локальным минимумом обеих эмпирических функций потерь $\mathcal{L}_{k}(\boldsymbol{\theta})$ и $\mathcal{L}_{k+1}(\boldsymbol{\theta})$, т.е.
    \[
        \nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*) = \nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*) = \mathbf{0}.
    \]
\end{assumption}
Предположение \ref{chapter:complex:assumption-local-optima-not-change} допускает следующую интерпретацию: новый объект данных является \textit{репрезентативным} для уже обученной модели, то есть он не приносит новой информации, а лишь уточняет ее.
При асимптотически большом объеме выборки данное свойство согласуется с эмпирическими результатами.

Воспользуемся квадратичным приближением Тейлора для упомянутых выше функций потерь в окрестности точки $\boldsymbol{\theta}^*$. Предполагаем, что разложение до второго порядка будет достаточным для изучения локального поведения. Член первого порядка обращается в ноль, поскольку градиенты $\nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*)$ и $\nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*)$ равны нулю:
\begin{equation}\label{chapter:complex:equation-approx}
    \mathcal{L}_{k}(\boldsymbol{\theta}) \approx \mathcal{L}_{k}(\boldsymbol{\theta}^*) + \frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \mathbf{H}^{(k)}(\boldsymbol{\theta}^*) (\boldsymbol{\theta} - \boldsymbol{\theta}^*),
\end{equation}
где введено обозначение гессиана функции $\mathcal{L}_{k}(\boldsymbol{\theta})$ по параметрам $\boldsymbol{\theta}$ в точке $\boldsymbol{\theta}^*$ как $\mathbf{H}^{(k)}(\boldsymbol{\theta}^*) \in \mathbb{R}^{P \times P}$. Более того, полный гессиан может быть записан как среднее значение гессианов отдельных членов эмпирической функции потерь:
\[
    \mathbf{H}^{(k)}(\boldsymbol{\theta}) = \nabla^2_{\boldsymbol{\theta}} \mathcal{L}_{k}(\boldsymbol{\theta}) = \frac{1}{k} \sum\limits_{i=1}^{k} \nabla^2_{\boldsymbol{\theta}} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}).
\]

Следовательно, используя полученное квадратичное приближение~\eqref{chapter:complex:equation-approx}, формула для разности потерь~\eqref{chapter:complex:equation-difference} принимает вид:
\begin{align}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1} \left( \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right) +\\
    &\quad+ \frac{1}{k+1} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \left( \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right) (\boldsymbol{\theta} - \boldsymbol{\theta}^*),
\end{align}
причем, используя неравенство треугольника, получаем следующую оценку:
\begin{align}\label{chapter:complex:equation-mod-difference-full}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| &\leqslant \frac{1}{k+1} \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| +\\
    &\quad+ \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{align}

Первое слагаемое допускает ограничение константой, поскольку сама функция потерь принимает ограниченные значения. Выражение с гессианами требует более сложной оценки. Подробный анализ матриц Гессе для различных типов параметрических моделей глубокого обучения представлен в работах~\cite{grabovoi2024unraveling748584228, grabovoi2024convnets743111032}. Анализ локальной сходимости ландшафта функции потерь основан на ее матрице Гессе.

Получаем выражение для анализа, описывающее поведение ландшафта функции потерь:
\begin{equation}\label{chapter:complex:equation-mod-difference}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_{\ell}}{k+1}+ \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{equation}

Установлено, что анализ сходимости ландшафта оптимизационной задачи сводится к анализу нормы матрицы Гессе.
Оценка~\eqref{chapter:complex:equation-mod-difference} задает некоторое свойство параметрического семейства функций~$f$ на заданной выборке~$D$.
Определим данное свойство как условную сложность модели~$f$ на выборке~$D:$
\begin{equation}\label{chapter:complex:equantion-subcomplex-model}
    \mu_f(f|D) : \mathfrak{F} \to \mathbb{R}_+,
\end{equation}
причем, более подробно рассмотрим частный случай условной меры сложности параметрического семейства функций~$f$ вида:
\begin{equation}\label{chapter:complex:equantion-subcomplex-model-surface}
    \mu_f(f|D) = \mathsf{E}_{\mathbf{x}_i\in D}\left\| \mathbf{H}_{i}(\boldsymbol{\theta}^*) -  \mathsf{E}_{\mathbf{x}_i\in D}\mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{equation}

\begin{definition}\label{chapter:complex:definition-subcomplex-model}
    Условной сложностью параметрической модели~$f$ относительно заданной выборки~$D$ назовем отображение~\eqref{chapter:complex:equantion-subcomplex-model}.
\end{definition}

\begin{definition}[Ландшафтная мера сложности]\label{chapter:complex:definition-subcomplex-model-surface}
    Ландшафтной мерой сложности параметрической функции~$f$ назовем условную сложность параметрической модели~$f$, заданную выражением~\eqref{chapter:complex:equantion-subcomplex-model-surface}.
\end{definition}

Определение~\ref{chapter:complex:definition-subcomplex-model} описывает прикладный способ задания сложности на параметрических семействах функций в контексте оптимизации на заданных выборках.
Условная сложность модели~$\mu_f(f|D)$ характеризует сложность архитектуры модели~$f$ при ее обучении на выборке данных $D$ и позволяет количественно оценить степень соответствия модели данным. При этом слишком простая модель может недообучаться, а слишком сложная~--- переобучаться.

Ландшафтная мера сложности~\ref{chapter:complex:definition-subcomplex-model-surface} представляет собой явный вид условной сложности, основанный на анализе оптимизационного ландшафта функции потерь. Выражение~\eqref{chapter:complex:equantion-subcomplex-model-surface} содержательно указывает на степень изменения кривизны функции потерь в окрестности оптимума при добавлении нового объекта данных.

\section{Оценки ландшафтной меры сложности для различных архитектур}

В настоящем разделе получены конкретные оценки ландшафтной меры сложности для полносвязных и сверточных нейронных сетей. Анализ основан на теоретических оценках спектральных норм матриц Гессе, полученных в работах~\cite{grabovoi2024unraveling748584228, grabovoi2024convnets743111032}.

\subsection{Полносвязные нейронные сети}

В настоящем подразделе получены оценки ландшафтной меры сложности для полносвязных нейронных сетей, являющихся базовой архитектурой глубокого обучения. Анализ основан на теоретических оценках сходимости ландшафта функции потерь, полученных в работе~\cite{grabovoi2024unraveling748584228}, где показано, что сходимость происходит со скоростью $O(L(hM)^{2L}/k)$ при увеличении размера выборки $k$, где $L$~--- число слоев, $h$~--- размер скрытого слоя, $M$~--- константа, ограничивающая параметры и данные.

На основе теоретических оценок сходимости ландшафта, полученных в работе~\cite{grabovoi2024unraveling748584228}, и использованных в них оценок нормы матрицы Гессе, которая имеет полиномиальную зависимость от размера слоя и экспоненциальную зависимость от числа слоев, получается выражение для ландшафтной меры полносвязной нейросетевой модели глубокого обучения.

\begin{lemma}[Асимптотика ландшафтной меры для полносвязных сетей]\label{chapter:complex:corollary-theorem-kiselev-loss}
    Ландшафтная мера сложности параметрической функции~$f$ полносвязной нейросетевой модели глубокого обучения имеет асимптотику:
    \begin{equation}\label{eq:landscape-fc}
        \mu_f(f|D) \propto L(hM)^{2L},
    \end{equation}
    где~$L$~--- число слоев,~$h$~--- размер скрытого слоя,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках сходимости ландшафта функции потерь и оценках нормы матрицы Гессе для полносвязных нейронных сетей, полученных в работе~\cite{grabovoi2024unraveling748584228}. 

Согласно определению ландшафтной меры сложности (определение~\ref{chapter:complex:definition-subcomplex-model-surface}), ландшафтная мера определяется через спектральную норму матрицы Гессе. В работе~\cite{grabovoi2024unraveling748584228} установлено, что норма матрицы Гессе для полносвязных сетей имеет асимптотику $\|\mathbf{H}_i(\boldsymbol{\theta})\|_2 \propto L(hM)^{2L}$, где $L$~--- число слоев, $h$~--- размер скрытого слоя, $M$~--- константа, ограничивающая параметры и данные.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме самого гессиана, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

Лемма~\ref{chapter:complex:corollary-theorem-kiselev-loss} устанавливает, что ландшафтная мера сложности для полносвязных сетей экспоненциально зависит от глубины сети и полиномиально~--- от ширины слоев. Следовательно, для более глубоких сетей требуется экспоненциально больше данных для стабилизации ландшафта, что согласуется с представлением о том, что сложные модели требуют большего объема данных для обучения.

\subsection{Сверточные нейронные сети}

В настоящем подразделе получены оценки ландшафтной меры сложности для сверточных нейронных сетей, широко применяющихся в задачах обработки последовательностей и изображений. Анализ основан на теоретических оценках сходимости ландшафта функции потерь, полученных в работе~\cite{grabovoi2024convnets743111032}.

Начнем с анализа 1D-сверточных сетей, применяющихся в обработке последовательностей, временных рядов и сигналов. На основе теоретических оценок сходимости ландшафта для 1D-сверточных сетей, полученных в работе~\cite{grabovoi2024convnets743111032}, где показано, что сходимость происходит со скоростью $O(L(C^2M^2kd)^{L}/k)$, получается выражение для ландшафтной меры.

\begin{lemma}[Асимптотика ландшафтной меры для 1D-сверточных сетей]\label{chapter:complex:corollary-theorem-1Dconv-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 1D-сверточной модели глубокого обучения имеет асимптотику:
    \begin{equation}\label{eq:landscape-1dconv}
        \mu_f(f|D) \propto L(C^2M^2kd)^{L},
    \end{equation}
    где~$C$~--- максимальное число каналов,~$d$~--- длина входной последовательности,~$k$~--- размер свертки,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках нормы матрицы Гессе для 1D-сверточных нейронных сетей, полученных в работе~\cite{grabovoi2024convnets743111032}.

Согласно определению ландшафтной меры сложности (определение~\ref{chapter:complex:definition-subcomplex-model-surface}), ландшафтная мера определяется через спектральную норму матрицы Гессе. В работе~\cite{grabovoi2024convnets743111032} установлено, что норма матрицы Гессе для 1D-сверточных сетей имеет асимптотику, пропорциональную $L(C^2M^2kd)^{L}$, где $C$~--- максимальное число каналов, $d$~--- длина входной последовательности, $k$~--- размер свертки, $M$~--- константа, ограничивающая параметры и данные.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме самого гессиана, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

Лемма~\ref{chapter:complex:corollary-theorem-1Dconv-loss} демонстрирует, что сложность 1D-сверточных нейросетевых моделей экспоненциально зависит от глубины $L$ и полиномиально~--- от остальных гиперпараметров архитектуры. Особенностью 1D-архитектур является линейная зависимость от длины последовательности $d$, что отражает специфику обработки последовательностей и отличает их от полносвязных сетей, где такая зависимость отсутствует.

Перейдем к анализу 2D-сверточных сетей, применяющихся в задачах обработки изображений. На основе теоретических оценок сходимости ландшафта для 2D-сверточных сетей, полученных в работе~\cite{grabovoi2024convnets743111032}, где показано, что сходимость происходит со скоростью $O(C^2k^2L(C^2k^2M^2mn)^{L}/k)$, получается выражение для ландшафтной меры.

\begin{lemma}[Асимптотика ландшафтной меры для 2D-сверточных сетей]\label{chapter:complex:corollary-theorem-2Dconv-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 2D-сверточной модели глубокого обучения имеет асимптотику:
    \begin{equation}\label{eq:landscape-2dconv}
        \mu_f(f|D) \propto C^2k^2L(C^2k^2M^2mn)^{L},
    \end{equation}
    где~$C$~--- максимальное число каналов,~$m,n$~--- размеры входного изображения,~$k$~--- размер свертки,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках нормы матрицы Гессе для 2D-сверточных нейронных сетей, полученных в работе~\cite{grabovoi2024convnets743111032}.

Согласно определению ландшафтной меры сложности (определение~\ref{chapter:complex:definition-subcomplex-model-surface}), ландшафтная мера определяется через спектральную норму матрицы Гессе. В работе~\cite{grabovoi2024convnets743111032} установлено, что норма матрицы Гессе для 2D-сверточных сетей имеет асимптотику, пропорциональную $C^2k^2L(C^2k^2M^2mn)^{L}$, где $C$~--- максимальное число каналов, $m,n$~--- размеры входного изображения, $k$~--- размер свертки, $M$~--- константа, ограничивающая параметры и данные.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме самого гессиана, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

Лемма~\ref{chapter:complex:corollary-theorem-2Dconv-loss} устанавливает, что для 2D-сверточных сетей наблюдается более быстрый рост сложности по сравнению с 1D-архитектурами, что обусловлено двумерной природой данных: зависимость от размеров изображения $m \times n$ является квадратичной, в отличие от линейной зависимости от длины последовательности в 1D-сверточных сетях.

\subsection{Сверточные сети с операциями пулинга}

В настоящем подразделе получены оценки ландшафтной меры сложности для сверточных сетей, содержащих слои MaxPooling и AvgPooling. Операции пулинга широко применяются в современных архитектурах сверточных сетей для уменьшения размерности признакового пространства и снижения вычислительной сложности. Анализ основан на теоретических оценках нормы матрицы Гессе для сетей с операциями пулинга, полученных в работе~\cite{grabovoi2024convnets743111032}.

На основе теоретических оценок нормы матрицы Гессе для сверточных сетей с операциями MaxPooling и AvgPooling, полученных в работе~\cite{grabovoi2024convnets743111032}, где показано, что норма гессиана уменьшается за счет множителя $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$, где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $L$~--- глубина сети, $l$~--- позиция слоя пулинга, получается выражение для ландшафтной меры.

\begin{lemma}[Асимптотика ландшафтной меры для сверточных сетей с MaxPooling]\label{chapter:complex:corollary-theorem-maxpool-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 2D-сверточной модели глубокого обучения, содержащей слой MaxPool2D в позиции~$l$, имеет асимптотику:
    \begin{equation}\label{eq:landscape-maxpool}
        \mu_f(f|D) \propto \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L},
    \end{equation}
    где~$C$~--- максимальное число каналов,~$m,n$~--- размеры входного изображения,~$k$~--- размер свертки,~$k_{\mathrm{pool}}$~--- размер ядра пулинга,~$L$~--- глубина сети,~$l$~--- позиция слоя пулинга,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках нормы матрицы Гессе для сверточных нейронных сетей с операцией MaxPooling, полученных в работе~\cite{grabovoi2024convnets743111032}.

Согласно определению ландшафтной меры сложности (определение~\ref{chapter:complex:definition-subcomplex-model-surface}), ландшафтная мера определяется через спектральную норму матрицы Гессе. В работе~\cite{grabovoi2024convnets743111032} установлено, что норма матрицы Гессе для 2D-сверточных сетей с операцией MaxPool2D в позиции $l$ имеет асимптотику, пропорциональную $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L}$, где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $L$~--- глубина сети, $l$~--- позиция слоя пулинга.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме самого гессиана, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

\begin{lemma}[Асимптотика ландшафтной меры для сверточных сетей с AvgPooling]\label{chapter:complex:corollary-theorem-avgpool-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 2D-сверточной модели глубокого обучения, содержащей слой AvgPool2D в позиции~$l$, имеет асимптотику:
    \begin{equation}\label{eq:landscape-avgpool}
        \mu_f(f|D) \propto \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L},
    \end{equation}
    где~$C$~--- максимальное число каналов,~$m,n$~--- размеры входного изображения,~$k$~--- размер свертки,~$k_{\mathrm{pool}}$~--- размер ядра пулинга,~$L$~--- глубина сети,~$l$~--- позиция слоя пулинга,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках нормы матрицы Гессе для сверточных нейронных сетей с операцией AvgPooling, полученных в работе~\cite{grabovoi2024convnets743111032}.

Согласно определению ландшафтной меры сложности (определение~\ref{chapter:complex:definition-subcomplex-model-surface}), ландшафтная мера определяется через спектральную норму матрицы Гессе. В работе~\cite{grabovoi2024convnets743111032} установлено, что норма матрицы Гессе для 2D-сверточных сетей с операцией AvgPool2D в позиции $l$ имеет асимптотику, пропорциональную $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L}$, где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $L$~--- глубина сети, $l$~--- позиция слоя пулинга.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме самого гессиана, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

Леммы~\ref{chapter:complex:corollary-theorem-maxpool-loss} и~\ref{chapter:complex:corollary-theorem-avgpool-loss} демонстрируют, что операции MaxPooling и AvgPooling оказывают аналогичное регуляризирующее влияние на сложность оптимизационного ландшафта, уменьшая ландшафтную меру сложности за счет множителя $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$. Данный множитель отражает уменьшение размерности признакового описания после операции пулинга, что приводит к сокращению эффективной размерности параметрического пространства. Степень $L-l+2$ показывает, что влияние пулинга распространяется на все последующие слои: чем раньше расположен слой пулинга, тем сильнее его редуцирующее воздействие на ландшафтную меру сложности. Следовательно, операции пулинга частично компенсируют экспоненциальный рост сложности с увеличением глубины сети, что демонстрирует их регуляризирующий эффект.

\subsection{Сравнительный анализ ландшафтной меры сложности}

В таблице~\ref{tab:landscape-comparison} представлено сравнение асимптотических оценок ландшафтной меры сложности для различных архитектур нейронных сетей.

\begin{table}[h!t]
\centering
\caption{Сравнение асимптотических оценок ландшафтной меры сложности для различных архитектур}
\label{tab:landscape-comparison}
\begin{tabular}{lp{4.5cm}p{6.5cm}}
\toprule
Архитектура & Ландшафтная мера $\mu_f(f|D)$ & Зависимость от параметров \\
\midrule
Полносвязная сеть & $\propto L(hM)^{2L}$ & Экспоненциальная от глубины $L$; полиномиальная от ширины $h$ \\
\midrule
1D-сверточная сеть & $\propto L(C^2M^2kd)^{L}$ & Экспоненциальная от глубины $L$; полиномиальная от каналов $C$, размера ядра $k$; линейная от длины последовательности $d$ \\
\midrule
2D-сверточная сеть & $\propto C^2k^2L(C^2k^2M^2mn)^{L}$ & Экспоненциальная от глубины $L$; полиномиальная от каналов $C$, размера ядра $k$; квадратичная от размеров изображения $m \times n$ \\
\midrule
2D-сверточная сеть с MaxPooling & $\propto \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L}$ & Экспоненциальная от глубины $L$; регуляризация через множитель $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$; зависимость от позиции слоя пулинга $l$ \\
\midrule
2D-сверточная сеть с AvgPooling & $\propto \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L}$ & Экспоненциальная от глубины $L$; регуляризация через множитель $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$; зависимость от позиции слоя пулинга $l$ \\
\bottomrule
\end{tabular}
\end{table}

Анализ таблицы~\ref{tab:landscape-comparison} позволяет выявить следующие закономерности.

Для всех рассмотренных архитектур ландшафтная мера сложности экспоненциально зависит от числа слоев $L$, что является общим свойством всех типов нейронных сетей. Следовательно, увеличение глубины сети приводит к экспоненциальному росту сложности оптимизационного ландшафта, что требует экспоненциально большего объема данных для стабилизации функции потерь. Параметры архитектуры, отличные от глубины (ширина слоя, число каналов, размер ядра), входят в оценку полиномиально, что указывает на более умеренное влияние этих параметров на сложность ландшафта по сравнению с глубиной сети.

Полносвязные сети характеризуются тем, что ландшафтная мера зависит только от глубины $L$ и ширины $h$ скрытых слоев, не учитывая размерность входных данных. Это отражает универсальность полносвязных сетей, способных обрабатывать данные произвольной размерности после их векторизации. Ландшафтная мера для полносвязных сетей имеет асимптотику, пропорциональную $L(hM)^{2L}$, демонстрируя экспоненциальную зависимость от глубины и полиномиальную зависимость от ширины слоев.

В отличие от полносвязных сетей, 1D-сверточные сети характеризуются тем, что ландшафтная мера включает линейную зависимость от длины входной последовательности $d$. Ландшафтная мера для 1D-сверточных сетей имеет асимптотику, пропорциональную $L(C^2M^2kd)^{L}$, где $C$~--- максимальное число каналов, $k$~--- размер свертки. Следовательно, для обработки более длинных последовательностей требуется пропорционально больше данных для стабилизации ландшафта.

Для 2D-сверточных сетей ландшафтная мера включает квадратичную зависимость от размеров входного изображения $m \times n$, что отражает двумерную природу данных. Ландшафтная мера для 2D-сверточных сетей имеет асимптотику, пропорциональную $C^2k^2L(C^2k^2M^2mn)^{L}$, демонстрируя более быстрый рост сложности по сравнению с 1D-архитектурами при увеличении размеров входных данных.

Сверточные сети с операциями MaxPooling и AvgPooling характеризуются тем, что ландшафтная мера включает дополнительный регуляризирующий множитель $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$, который уменьшает сложность оптимизационного ландшафта. Ландшафтная мера для таких сетей имеет асимптотику, пропорциональную $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L}$, где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $l$~--- позиция слоя пулинга. Влияние пулинга зависит от его позиции в сети: чем раньше расположен слой пулинга, тем сильнее его редуцирующее воздействие. Операции пулинга частично компенсируют экспоненциальный рост сложности с увеличением глубины сети, демонстрируя регуляризирующий эффект.

Полученные оценки позволяют количественно оценить влияние различных гиперпараметров архитектуры на сложность оптимизационного ландшафта и, следовательно, на требования к объему обучающих данных. Для глубоких сетей (большие значения $L$) экспоненциальный рост сложности означает, что даже небольшое увеличение глубины требует существенного увеличения объема данных. Напротив, увеличение ширины слоев или числа каналов приводит к полиномиальному росту сложности, что делает эти параметры более управляемыми с точки зрения требований к данным.

Сравнение полносвязных и сверточных архитектур показывает, что сверточные сети более эффективно используют структуру данных (пространственную или временную), что отражается в явной зависимости ландшафтной меры от размеров входных данных. Это согласуется с практическим опытом, показывающим, что сверточные сети часто требуют меньшего объема данных для достижения заданного качества по сравнению с полносвязными сетями при работе со структурированными данными. Анализ влияния операций пулинга демонстрирует их регуляризирующую роль в глубоких сверточных сетях: пулинг не только уменьшает вычислительную сложность, но и снижает сложность оптимизационного ландшафта, что способствует более стабильному обучению. Практическое значение данного результата заключается в возможности целенаправленного размещения слоев пулинга для управления сложностью модели и требованиями к объему обучающих данных.

\section{Заключение}

В настоящей работе разработан единый теоретический аппарат для формализации соотношения между сложностью модели и сложностью данных в контексте обучения глубоких нейронных сетей.

Введены формальные определения меры сложности выборки и меры сложности модели в рамках теории мер, а также установлен критерий обучаемости модели на выборке, согласно которому сложность модели не должна превышать сложности данных. Введены понятия условной сложности модели и ландшафтной меры сложности, определяемой через спектральные свойства матриц Гессе функции потерь. Доказана теорема~\ref{chapter:complex:theorem-finetunning}, устанавливающая необходимое условие дообучения модели на расширенной выборке данных.

На основе теоретических оценок нормы матрицы Гессе, полученных в работах~\cite{grabovoi2024unraveling748584228, grabovoi2024convnets743111032}, установлены асимптотики ландшафтной меры сложности для различных архитектур: для полносвязных сетей (лемма~\ref{chapter:complex:corollary-theorem-kiselev-loss}), для 1D- и 2D-сверточных сетей (леммы~\ref{chapter:complex:corollary-theorem-1Dconv-loss} и~\ref{chapter:complex:corollary-theorem-2Dconv-loss}), а также для сверточных сетей с операциями MaxPooling и AvgPooling (леммы~\ref{chapter:complex:corollary-theorem-maxpool-loss} и~\ref{chapter:complex:corollary-theorem-avgpool-loss}). Проведен сравнительный анализ ландшафтной меры сложности для различных архитектур, результаты которого представлены в таблице~\ref{tab:landscape-comparison}.


\bibliography{main}

\end{document}