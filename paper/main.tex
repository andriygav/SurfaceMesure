\documentclass[
aps,%
12pt,%
final,%
notitlepage,%
oneside,%
onecolumn,%
nobibnotes,%
nofootinbib,% 
superscriptaddress,%
noshowpacs,%
amsmath,%
amssymb,%
centertags]%
{revtex4-2}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}

\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{booktabs}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{definition}{Определение}
\newtheorem{assumption}{Предположение}
\newtheorem{property}{Свойство}
\newtheorem{corollary}{Следствие}
\newtheorem{remark}{Замечание}
\newtheorem{hypothesis}{Гипотеза}

\begin{document}

\title{ЛАНДШАФТНАЯ МЕРА МОДЕЛЕЙ ГЛУБОКОГО ОБУЧЕНИЯ}

\author{\firstname{А.~В.}~\surname{Грабовой}}
\email{grabovoy@ipu.ru}
\affiliation{%
    Институт проблем управления им. В. А. Трапезникова Российской академии наук (ИПУ РАН)
}%

\begin{abstract}
    Оценка сложности моделей машинного обучения является одной из основных задач при выборе моделей для решения 
    прикладных задач.
    Спектральные свойства матриц Гессе позволяют количественно описывать кривизну оптимизационного ландшафта и характеризовать сложность настройки параметров нейросетевых моделей.
    В работе вводится ландшафтная мера сложности модели, определяемая через спектральные свойства матриц Гессе функции потерь и описывающая изменение кривизны ландшафта при добавлении новых объектов данных.
    На основе теоретических оценок спектральных норм матриц Гессе получены асимптотические оценки ландшафтной меры сложности для полносвязных, 1D- и 2D-сверточных нейронных сетей, а также для сверточных сетей с операциями MaxPooling и AvgPooling, демонстрирующие экспоненциальную зависимость от глубины сети и полиномиальную зависимость от архитектурных параметров.
    Предложенный подход обеспечивает вычислительно осуществимые методы оценки ландшафтной сложности моделей глубокого обучения на основе аналитических оценок спектральных свойств матриц Гессе.

    \vspace{1em}
    \textbf{Ключевые слова:} глубокое обучение, нейронные сети, ландшафт функции потерь, матрица Гессе, ландшафтная мера сложности, асимптотические оценки, обобщающая способность.
\end{abstract}

\maketitle

\section{Введение}

Современные подходы к анализу сложности моделей глубокого обучения имеют существенные ограничения, связанные либо несостоятельностью оценок для моделей глубокого обучения либо с эмпирическим построением зависимости без теоретического обоснования.
Классические меры сложности~\cite{mcallester2013book} не учитывают специфику перепараметризованных нейронных сетей.
С другой стороны, сложность модели и данных рассматриваются изолированно без формальных критериев их соответствия.
Анализ матриц Гессе остается в основном эмпирическим и не обеспечивает строгих теоретических связей для конкретных архитектур моделей глубокого обучения.
В результате отсутствует единый математический аппарат, способный связать сложность модели и данных в рамках единой теоретической базы, применимой к широкому классу архитектур нейронных сетей.

Современные исследования посвящены анализу ландшафта функции потерь и спектральных свойств матриц Гессе как меры сложности нейросетевых моделей~\cite{sagun2018empiricalanalysishessianoverparametrized}.
Эмпирические исследования показали, что матрицы Гессе перепараметризованных нейронных сетей обладают характерной структурой: большинство собственных значений сосредоточены около нуля, а небольшое число больших собственных значений определяет кривизну ландшафта~\cite{pmlr-v97-ghorbani19b}.
Минимумы с малым спектральным радиусом Гессиана коррелируют с лучшей обобщающей способностью~\cite{keskar2016large,dinh2017sharp}.
Однако большинство результатов остаются эмпирическими и зависят от метода оптимизации, архитектуры и данных, на которых проводится настройка параметров модели.

Для матричных моделей глубокого обучения получены оценки на матрицы Гессе, а также на их нормы~\cite{sagun2018empiricalanalysishessianoverparametrized,skorski2019chainruleshessianhigher,grabovoi2024unraveling748584228,grabovoi2024convnets743111032}. Для полносвязных нейронных сетей~\cite{grabovoi2024unraveling748584228} получены оценки спектральной нормы матрицы Гессе, которые указывают, что верхняя оценка нормы имеет экспоненциальную зависимость от глубины сети и полиномиальную зависимость от ширины слоев.
Для сверточных нейронных сетей~\cite{grabovoi2024convnets743111032} получены схожие оценки нормы.
Например для одномерных сверточных сетей установлена оценка, демонстрирующая мультипликативную зависимость от глубины и полиномиально-экспоненциальную зависимость от числа каналов, размера ядра и длины последовательности.
Для двумерных сверточных сетей получены аналогичные оценки, учитывающие двумерную природу данных.

Анализ матриц Гессе для моделей глубокого обучения проводится на основе декомпозиции матрицы Гессе на G-компоненту и H-компоненту~\cite{sagun2018empiricalanalysishessianoverparametrized,skorski2019chainruleshessianhigher}.
G-компонента отражает кривизну функции потерь относительно выходов сети, тогда как H-компонента описывает кривизну самой нейронной сети.
Эта декомпозиция позволяет анализировать вклад различных факторов в общую сложность оптимизационного ландшафта и связывает свойства матрицы Гессе с обобщающей способностью моделей.
Визуализация ландшафта функции потерь~\cite{li2018visualizing} эмпирически исследует геометрию пространства параметров при их выборе.
Структурный анализ матриц Гессе на больших масштабах~\cite{papyan2019spectrumdeepnethessiansscale,pmlr-v97-ghorbani19b} задает динамику спектра Гессиана в процессе обучения и его зависимость от размера выборки.
Для сверточных нейронных сетей получены теоретические оценки структуры матриц Гессе~\cite{singh2023hessianperspectivenatureconvolutional}, которая описывается Теплицевыми матрицами.
Для трансформеров проведен теоретический анализ Гессиана~\cite{ormaniec2024attentionhessian}, связывающий его структуру с механизмами внимания, причем архитектурные особенности отражаются в спектральных свойствах матрицы Гессе.

Теоретические результаты о структуре ландшафта~\cite{pennington2017spectrum,pennington2018emergence} указывают на то, что градиентный спуск находит глобальные минимумы для достаточно широких сетей~\cite{gurari2018gradient}, что связано с уменьшением кривизны ландшафта в пределе бесконечной ширины.
В пределе бесконечной ширины нейронные сети демонстрируют поведение, описываемое теорией нейронного тангенциального ядра (англ. NTK)~\cite{NEURIPS2018_5a4be1fa}.
Исследования крупномасштабной структуры ландшафта~\cite{fort2019large,fort2019energy} показали наличие иерархии минимумов и связь между геометрией ландшафта и обобщающей способностью.

Для больших языковых моделей все чаще используются эмпирические законы масштабирования~\cite{kaplan2020ScalingLaws}, которые указывают на то, что функция потерь языковых моделей подчиняется степенным законам относительно числа параметров, объема обучающих данных и вычислительного бюджета.
Эти зависимости показывают, что для достижения заданного уровня качества необходимо синхронно масштабировать как модель, так и данные, причем влияние данных оказывается несколько сильнее.
Было формализовано оптимальное соотношение между числом параметров и объемом данных~\cite{hoffmann2022Chinchila}, демонстрирующее критическую важность баланса между сложностью модели и объемом данных.

Несмотря на значительный прогресс в получении теоретических оценок спектральных норм матриц Гессе для различных архитектур~\cite{grabovoi2024unraveling748584228,grabovoi2024convnets743111032} и эмпирических закономерностей~\cite{kaplan2020ScalingLaws,hoffmann2022Chinchila}, строгие теоретические связи между спектральными свойствами Гессиана и обобщающей способностью установлены лишь для ограниченных классов моделей.
Отсутствует единый формализм, связывающий эти оценки с мерой сложности данных и устанавливающий формальные критерии обучаемости модели на выборке.
Для моделей с миллионами и миллиардами параметров прямое вычисление и анализ матриц Гессе становится непрактичным из-за квадратичной сложности по памяти и вычислениям, что мотивирует разработку аналитических методов оценки сложности.

В настоящей же работе представлен теоретический формализм, устанавливающий  соотношение между сложностью модели и сложностью данных для глубоких нейронных сетей.
В работе введено понятие ландшафтной меры сложности, определяемой через спектральные свойства матриц Гессе функции потерь и характеризующей изменение кривизны оптимизационного ландшафта при добавлении новых объектов данных.
На основе теоретических оценок спектральных норм матриц Гессе для различных архитектур получены асимптотические оценки ландшафтной меры сложности для полносвязных, 1D- и 2D-сверточных сетей, а также для сверточных сетей с операциями MaxPooling и AvgPooling.


\section{Ландшафтная мера моделей глубокого обучения}\label{chapter:complexity:loss}

Пусть задано множество параметрических аппроксимирующих моделей
\[
    \mathfrak{F} = \left\{f_i\right\},
\]
где каждое~$f_i$ является множеством параметрических аппроксимирующих функций.
\begin{definition}[Мера сложности модели]\label{chapter:complex:def-model}
    Мерой сложности модели~$f$ назовем отображение~$\mu_f(f_i)$:
    \[
        \mu_f(f_i) : \mathfrak{F} \to \mathbb{R}_+.
    \]
\end{definition}

Определение меры сложности модели~$f$ не является определением меры в общем случае, так как множество~$\mathfrak{F}$ не является кольцом.
Поэтому данная мера представляет собой некоторое отображение, которое является характеристикой сложности.
Примером меры сложности модели, удовлетворяющей определению~\ref{chapter:complex:def-model}, является число параметров модели.

Пусть задана выборка из простой генеральной совокупности~$\Gamma_C$:
\begin{equation*}
    D = \left\{ (\mathbf{x}_i, \mathbf{y}_i) \right\}, \quad i = 1, \ldots, m, \quad \mathbf{x} \in \mathcal{X}, \ \mathbf{y} \in \mathcal{Y}, \quad D\subset \Gamma_C.
\end{equation*}
Рассмотрим некоторое параметрическое отображение~$f_{\boldsymbol{\theta}}: \mathcal{X} \to \mathcal{Y},$ которое аппроксимирует условное распределение целевой переменной для заданного признакового описания объекта~$p(\mathbf{y}|\mathbf{x}).$ Параметры~$\boldsymbol{\theta}$ функции~$f_{\boldsymbol{\theta}}$ принадлежат пространству~$\mathbb{R}^{P},$ где~$P$ описывает число параметров отображения~$f_{\boldsymbol{\theta}}$.
Пусть, для выбора оптимального вектора параметров~$\hat{\boldsymbol{\theta}}$ используется подход минимизации эмпирического риска:
\begin{equation*}
    \hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} \mathcal{L}_m(\boldsymbol{\theta}),
\end{equation*}
где функция эмпирического риска для выборки размера~$|D|=m$ задается в следующем виде: 
\begin{equation*}
    \mathcal{L}_m(\boldsymbol{\theta}) = \frac{1}{m} \sum\limits_{i=1}^{m} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_i), \mathbf{y}_i) \approx \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim p(\mathbf{x}, \mathbf{y})} \left[ \ell(f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y}) \right],
\end{equation*}
где функция~$\ell\left(\mathbf{z}, \mathbf{y}\right)$ описывает ошибку на одном объекте.
Далее в качестве функции~$\ell$ будут рассматриваться либо кросс-энтропийная функция потерь, либо средняя квадратическая ошибка, в зависимости от рассматриваемой задачи и архитектуры модели.

Функция эмпирического риска~$\mathcal{L}_m(\boldsymbol{\theta})$ задает некоторую поверхность в пространстве параметров размерности~$P$.
Изменение данной поверхности при добавлении новых объектов данных задает количественную оценку влияние объема выборки на оптимизационный ландшафт.
Изменение значения функции потерь при добавлении одного объекта вычисляется следующим образом:
\begin{align}\label{chapter:complex:equation-difference}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1}\sum_{i=1}^{k+1}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) - \frac{1}{k}\sum_{i=1}^k\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \\
    &= \frac{1}{k+1}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1})-\sum_{i=1}^{k}\frac{1}{k(k+1)}\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \\
    &= \frac{1}{k+1} \left(\ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_{k}(\boldsymbol{\theta})\right).
\end{align}
Особый интерес представляют предельные свойства при стремлении размера выборки к бесконечности.
Для дальнейших оценок данной разности вводится предположение~\ref{chapter:complex:assumption-local-optima-not-change}, которое подтверждается на практике~\cite{grabovoi2024unraveling748584228}.
\begin{assumption}\label{chapter:complex:assumption-local-optima-not-change}
    Пусть $\boldsymbol{\theta}^*$ является локальным минимумом обеих эмпирических функций потерь $\mathcal{L}_{k}(\boldsymbol{\theta})$ и $\mathcal{L}_{k+1}(\boldsymbol{\theta})$, т.е.
    \[
        \nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*) = \nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*) = \mathbf{0}.
    \]
\end{assumption}

Воспользуемся квадратичным приближением Тейлора для эмпирической функции потерь в окрестности точки $\boldsymbol{\theta}^*$.
Член первого порядка обращается в ноль, поскольку градиенты $\nabla \mathcal{L}_{k}(\boldsymbol{\theta}^*)$ и $\nabla \mathcal{L}_{k+1}(\boldsymbol{\theta}^*)$ равны нулю:
\begin{equation}\label{chapter:complex:equation-approx}
    \mathcal{L}_{k}(\boldsymbol{\theta}) \approx \mathcal{L}_{k}(\boldsymbol{\theta}^*) + \frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \mathbf{H}^{(k)}(\boldsymbol{\theta}^*) (\boldsymbol{\theta} - \boldsymbol{\theta}^*),
\end{equation}
где введено обозначение матрицы Гессе функции $\mathcal{L}_{k}(\boldsymbol{\theta})$ по параметрам $\boldsymbol{\theta}$ в точке $\boldsymbol{\theta}^*$ как $\mathbf{H}^{(k)}(\boldsymbol{\theta}^*) \in \mathbb{R}^{P \times P}$.
Более того, полная матрица Гессе записывается как среднее значение матриц Гессе отдельных членов эмпирической функции потерь:
\[
    \mathbf{H}^{(k)}(\boldsymbol{\theta}) = \nabla^2_{\boldsymbol{\theta}} \mathcal{L}_{k}(\boldsymbol{\theta}) = \frac{1}{k} \sum\limits_{i=1}^{k} \nabla^2_{\boldsymbol{\theta}} \ell(f_{\boldsymbol{\theta}}(\mathbf{x}_{i}), \mathbf{y}_{i}) = \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}).
\]
Следовательно, используя полученное квадратичное приближение~\eqref{chapter:complex:equation-approx}, формула для разности потерь~\eqref{chapter:complex:equation-difference} принимает вид:
\begin{align*}
    \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) &= \frac{1}{k+1} \left( \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right) +\\
    &\quad+ \frac{1}{k+1} (\boldsymbol{\theta} - \boldsymbol{\theta}^*)^\top \left( \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right) (\boldsymbol{\theta} - \boldsymbol{\theta}^*),
\end{align*}
причем, используя неравенство треугольника, получаем следующую оценку:
\begin{align}\label{chapter:complex:equation-mod-difference-full}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| &\leqslant \frac{1}{k+1} \left| \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \frac{1}{k} \sum\limits_{i=1}^{k} \ell(f_{\boldsymbol{\theta}^*}(\mathbf{x}_{i}), \mathbf{y}_{i}) \right| +\\
    &\quad+ \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{align}

Первое слагаемое допускает ограничение константой, поскольку сама функция потерь принимает ограниченные значения.
Выражение с матрицей Гессе требует более сложной оценки.
Соответственно получааем, что анализ локальной сходимости ландшафта функции потерь основан на ее матрице Гессе:
\begin{equation}\label{chapter:complex:equation-mod-difference}
    \left| \mathcal{L}_{k+1}(\boldsymbol{\theta}) - \mathcal{L}_k(\boldsymbol{\theta}) \right| \leqslant \frac{M_{\ell}}{k+1}+ \frac{1}{k+1} \left\|\boldsymbol{\theta} - \boldsymbol{\theta}^*\right\|_2^2 \left\| \mathbf{H}_{k+1}(\boldsymbol{\theta}^*) - \frac{1}{k} \sum\limits_{i=1}^{k} \mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{equation}

Тогда, анализ сходимости ландшафта оптимизационной задачи сводится к анализу нормы матрицы Гессе.
Оценка~\eqref{chapter:complex:equation-mod-difference} задает некоторое свойство параметрического семейства функций~$f$ на заданной выборке~$D$.
Определим данное свойство как условную сложность модели~$f$ на выборке~$D$:
\begin{equation}\label{chapter:complex:equantion-subcomplex-model}
    \mu_f(f|D) : \mathfrak{F} \to \mathbb{R}_+,
\end{equation}
причем, более подробно рассмотрим частный случай условной меры сложности параметрического семейства функций~$f$ вида:
\begin{equation}\label{chapter:complex:equantion-subcomplex-model-surface}
    \mu_f(f|D) = \mathsf{E}_{\mathbf{x}_i\in D}\left\| \mathbf{H}_{i}(\boldsymbol{\theta}^*) -  \mathsf{E}_{\mathbf{x}_i\in D}\mathbf{H}_{i}(\boldsymbol{\theta}^*) \right\|_2.
\end{equation}

\begin{definition}\label{chapter:complex:definition-subcomplex-model}
    Условной сложностью параметрической модели~$f$ относительно заданной выборки~$D$ назовем отображение~\eqref{chapter:complex:equantion-subcomplex-model}.
\end{definition}

\begin{definition}[Ландшафтная мера сложности]\label{chapter:complex:definition-subcomplex-model-surface}
    Ландшафтной мерой сложности параметрической функции~$f$ назовем условную сложность параметрической модели~$f$, заданную выражением~\eqref{chapter:complex:equantion-subcomplex-model-surface}.
\end{definition}

Определение~\ref{chapter:complex:definition-subcomplex-model} описывает прикладной способ задания сложности на параметрических семействах функций для заданных выборках.
Условная сложность модели~$\mu_f(f|D)$ характеризует сложность архитектуры модели~$f$ при ее обучении на выборке данных $D$ и количественно оценивает соответствие модели данным.

Ландшафтная мера сложности~\ref{chapter:complex:definition-subcomplex-model-surface} представляет собой условную сложность, основанную на анализе оптимизационного ландшафта функции потерь.
Выражение~\eqref{chapter:complex:equantion-subcomplex-model-surface} указывает на степень изменения кривизны функции потерь в окрестности оптимума при добавлении нового объекта данных.

\section{Оценки ландшафтной меры сложности для различных архитектур}

\subsection{Полносвязные нейронные сети}

Анализ основан на теоретических оценках сходимости ландшафта функции потерь, полученных в работе~\cite{grabovoi2024unraveling748584228}, где показано, что сходимость происходит со скоростью $O(L(hM)^{2L}/k)$ при увеличении размера выборки $k$, где $L$~--- число слоев, $h$~--- размер скрытого слоя, $M$~--- константа, ограничивающая параметры и данные.

На основе теоретических и эмпирических оценок сходимости ландшафта, полученных в работе~\cite{grabovoi2024unraveling748584228}, и использованных в них оценок нормы матрицы Гессе, которая имеет полиномиальную зависимость от размера слоя и экспоненциальную зависимость от числа слоев, получается выражение для ландшафтной меры полносвязной нейросетевой модели глубокого обучения.

\begin{lemma}[Асимптотика ландшафтной меры для полносвязных сетей]\label{chapter:complex:corollary-theorem-kiselev-loss}
    Ландшафтная мера сложности параметрической функции~$f$ полносвязной нейросетевой модели глубокого обучения имеет асимптотику:
    \begin{equation}\label{eq:landscape-fc}
        \mu_f(f|D) \propto L(hM)^{2L},
    \end{equation}
    где~$L$~--- число слоев,~$h$~--- размер скрытого слоя,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках сходимости ландшафта функции потерь и оценках нормы матрицы Гессе для полносвязных нейронных сетей, полученных в работе~\cite{grabovoi2024unraveling748584228}. 

Согласно определению~\ref{chapter:complex:definition-subcomplex-model-surface} ландшафтной меры сложности, ландшафтная мера определяется через спектральную норму матрицы Гессе.
В работе~\cite{grabovoi2024unraveling748584228} установлено, что норма матрицы Гессе для полносвязных сетей имеет асимптотику $\|\mathbf{H}_i(\boldsymbol{\theta})\|_2 \propto L(hM)^{2L}$, где $L$~--- число слоев, $h$~--- размер скрытого слоя, $M$~--- константа, ограничивающая параметры и данные.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме самой матрце Гессе, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

Лемма~\ref{chapter:complex:corollary-theorem-kiselev-loss} устанавливает, что ландшафтная мера сложности для полносвязных сетей экспоненциально зависит от глубины сети и полиномиально~--- от ширины слоев.
Следовательно, для более глубоких сетей требуется экспоненциально больше данных для стабилизации ландшафта, что согласуется с представлением о том, что сложные модели требуют большего объема данных для обучения.

\subsection{Сверточные нейронные сети}

Анализ основан на теоретических оценках сходимости ландшафта функции потерь, полученных в работе~\cite{grabovoi2024convnets743111032}.
На основе теоретических оценок сходимости ландшафта для 1D-сверточных сетей, полученных в работе~\cite{grabovoi2024convnets743111032}, где показано, что сходимость происходит со скоростью $O(L(C^2M^2kd)^{L})$, получается выражение для ландшафтной меры.

\begin{lemma}[Асимптотика ландшафтной меры для 1D-сверточных сетей]\label{chapter:complex:corollary-theorem-1Dconv-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 1D-сверточной модели глубокого обучения имеет асимптотику:
    \begin{equation}\label{eq:landscape-1dconv}
        \mu_f(f|D) \propto L(C^2M^2kd)^{L},
    \end{equation}
    где~$C$~--- максимальное число каналов,~$d$~--- длина входной последовательности,~$k$~--- размер свертки,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках нормы матрицы Гессе для 1D-сверточных нейронных сетей, полученных в работе~\cite{grabovoi2024convnets743111032}.

Согласно определению~\ref{chapter:complex:definition-subcomplex-model-surface} ландшафтной меры сложности, ландшафтная мера определяется через спектральную норму матрицы Гессе.
В работе~\cite{grabovoi2024convnets743111032} установлено, что норма матрицы Гессе для 1D-сверточных сетей имеет асимптотику, пропорциональную $L(C^2M^2kd)^{L}$, где $C$~--- максимальное число каналов, $d$~--- длина входной последовательности, $k$~--- размер свертки, $M$~--- константа, ограничивающая параметры и данные.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме самой матрицы Гессе, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

Лемма~\ref{chapter:complex:corollary-theorem-1Dconv-loss} демонстрирует, что сложность 1D-сверточных нейросетевых моделей экспоненциально зависит от глубины $L$ и полиномиально~--- от остальных гиперпараметров архитектуры.
Особенностью 1D-архитектур является линейная зависимость от длины последовательности $d$, что отражает специфику обработки последовательностей и отличает их от полносвязных сетей, где такая зависимость отсутствует.

Для 2D-сверточных сетей, применяющихся в задачах обработки изображений.
На основе теоретических оценок сходимости ландшафта для 2D-сверточных сетей, полученных в работе~\cite{grabovoi2024convnets743111032}, где показано, что сходимость происходит со скоростью $O(C^2k^2L(C^2k^2M^2mn)^{L})$, получается выражение для ландшафтной меры.

\begin{lemma}[Асимптотика ландшафтной меры для 2D-сверточных сетей]\label{chapter:complex:corollary-theorem-2Dconv-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 2D-сверточной модели глубокого обучения имеет асимптотику:
    \begin{equation}\label{eq:landscape-2dconv}
        \mu_f(f|D) \propto C^2k^2L(C^2k^2M^2mn)^{L},
    \end{equation}
    где~$C$~--- максимальное число каналов,~$m,n$~--- размеры входного изображения,~$k$~--- размер свертки,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках нормы матрицы Гессе для 2D-сверточных нейронных сетей, полученных в работе~\cite{grabovoi2024convnets743111032}.

Согласно определению~\ref{chapter:complex:definition-subcomplex-model-surface} ландшафтной меры сложности, ландшафтная мера определяется через спектральную норму матрицы Гессе.
В работе~\cite{grabovoi2024convnets743111032} установлено, что норма матрицы Гессе для 2D-сверточных сетей имеет асимптотику, пропорциональную $C^2k^2L(C^2k^2M^2mn)^{L}$, где $C$~--- максимальное число каналов, $m,n$~--- размеры входного изображения, $k$~--- размер свертки, $M$~--- константа, ограничивающая параметры и данные.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме матрицы Гессе, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

Лемма~\ref{chapter:complex:corollary-theorem-2Dconv-loss} устанавливает, что для 2D-сверточных сетей наблюдается более быстрый рост сложности по сравнению с 1D-архитектурами, что обусловлено двумерной природой данных: зависимость от размеров изображения $m \times n$ является квадратичной, в отличие от линейной зависимости от длины последовательности в 1D-сверточных сетях.

\subsection{Сверточные сети с операциями пулинга}

Операции пулинга широко применяются в современных архитектурах сверточных сетей для уменьшения размерности признакового пространства и снижения вычислительной сложности.
Анализ основан на теоретических оценках нормы матрицы Гессе для сетей с операциями пулинга, полученных в работе~\cite{grabovoi2024convnets743111032}.

На основе теоретических оценок нормы матрицы Гессе для сверточных сетей с операциями MaxPooling и AvgPooling, полученных в работе~\cite{grabovoi2024convnets743111032}, где показано, что норма гессиана уменьшается за счет множителя $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$, где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $L$~--- глубина сети, $l$~--- позиция слоя пулинга, получается выражение для ландшафтной меры.

\begin{lemma}[Асимптотика ландшафтной меры для сверточных сетей с MaxPooling]\label{chapter:complex:corollary-theorem-maxpool-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 2D-сверточной модели глубокого обучения, содержащей слой MaxPool2D в позиции~$l$, имеет асимптотику:
    \begin{equation}\label{eq:landscape-maxpool}
        \mu_f(f|D) \propto \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L},
    \end{equation}
    где~$C$~--- максимальное число каналов,~$m,n$~--- размеры входного изображения,~$k$~--- размер свертки,~$k_{\mathrm{pool}}$~--- размер ядра пулинга,~$L$~--- глубина сети,~$l$~--- позиция слоя пулинга,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках нормы матрицы Гессе для сверточных нейронных сетей с операцией MaxPooling, полученных в работе~\cite{grabovoi2024convnets743111032}.

Согласно определению~\ref{chapter:complex:definition-subcomplex-model-surface} ландшафтной меры сложности, ландшафтная мера определяется через спектральную норму матрицы Гессе.
В работе~\cite{grabovoi2024convnets743111032} установлено, что норма матрицы Гессе для 2D-сверточных сетей с операцией MaxPool2D в позиции $l$ имеет асимптотику, пропорциональную $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L}$, где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $L$~--- глубина сети, $l$~--- позиция слоя пулинга.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме матрицы Гессе, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

\begin{lemma}[Асимптотика ландшафтной меры для сверточных сетей с AvgPooling]\label{chapter:complex:corollary-theorem-avgpool-loss}
    Ландшафтная мера сложности параметрической функции~$f$ 2D-сверточной модели глубокого обучения, содержащей слой AvgPool2D в позиции~$l$, имеет асимптотику:
    \begin{equation}\label{eq:landscape-avgpool}
        \mu_f(f|D) \propto \left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L},
    \end{equation}
    где~$C$~--- максимальное число каналов,~$m,n$~--- размеры входного изображения,~$k$~--- размер свертки,~$k_{\mathrm{pool}}$~--- размер ядра пулинга,~$L$~--- глубина сети,~$l$~--- позиция слоя пулинга,~$M$~--- некоторая константа, ограничивающая параметры и данные.
\end{lemma}
\begin{proof}
Доказательство основано на теоретических оценках нормы матрицы Гессе для сверточных нейронных сетей с операцией AvgPooling, полученных в работе~\cite{grabovoi2024convnets743111032}.

Согласно определению~\ref{chapter:complex:definition-subcomplex-model-surface} ландшафтной меры сложности, ландшафтная мера определяется через спектральную норму матрицы Гессе.
В работе~\cite{grabovoi2024convnets743111032} установлено, что норма матрицы Гессе для 2D-сверточных сетей с операцией AvgPool2D в позиции $l$ имеет асимптотику, пропорциональную $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L}$, где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $L$~--- глубина сети, $l$~--- позиция слоя пулинга.

Поскольку ландшафтная мера сложности определяется как математическое ожидание нормы разности матриц Гессе отдельных объектов и среднего гессиана, и данная норма пропорциональна норме матрицы Гессе, получаем требуемую асимптотику ландшафтной меры сложности.
\end{proof}

Леммы~\ref{chapter:complex:corollary-theorem-maxpool-loss} и~\ref{chapter:complex:corollary-theorem-avgpool-loss} демонстрируют, что операции MaxPooling и AvgPooling оказывают аналогичное регуляризирующее влияние на сложность оптимизационного ландшафта, уменьшая ландшафтную меру сложности за счет множителя $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$.
Данный множитель отражает уменьшение размерности признакового описания после операции пулинга, что приводит к сокращению размерности параметрического пространства.
Степень $L-l+2$ показывает, что влияние пулинга распространяется на все последующие слои: чем раньше расположен слой пулинга, тем сильнее его редуцирующее воздействие на ландшафтную меру сложности.
Следовательно, операции пулинга частично компенсируют экспоненциальный рост сложности с увеличением глубины сети, что демонстрирует их регуляризирующий эффект.

\subsection{Сравнительный анализ ландшафтной меры сложности}

В таблице~\ref{tab:landscape-comparison} представлено сравнение асимптотических оценок ландшафтной меры сложности для различных архитектур нейронных сетей.

\begin{table}[h!t]
\centering
\caption{Сравнение асимптотических оценок ландшафтной меры сложности для различных архитектур}
\label{tab:landscape-comparison}
\begin{tabular}{|l|l|p{6.5cm}|}
\toprule
Модель & Ландшафтная мера $\mu_f(f|D)$ & Зависимость от параметров \\
\midrule
FNN & $\propto L(hM)^{2L}$ & Экспоненциальная от глубины $L$; полиномиальная от ширины $h$ \\
\midrule
1D-CNN & $\propto L(C^2M^2kd)^{L}$ & Экспоненциальная от глубины $L$; полиномиальная от каналов $C$, размера ядра $k$; линейная от длины последовательности $d$ \\
\midrule
2D-CNN & $\propto C^2k^2L(C^2k^2M^2mn)^{L}$ & Экспоненциальная от глубины $L$; полиномиальная от каналов $C$, размера ядра $k$; квадратичная от размеров изображения $m \times n$ \\
\midrule
2D-CNN+MaxPooling & $\propto C^2k^2L(C^2k^2M^2mn)^{L}\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$ & Экспоненциальная от глубины $L$; регуляризация через множитель $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$; зависимость от позиции слоя пулинга $l$ \\
\midrule
2D-CNN+AvgPooling & $\propto C^2k^2L(C^2k^2M^2mn)^{L}\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$ & Экспоненциальная от глубины $L$; регуляризация через множитель $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$; зависимость от позиции слоя пулинга $l$ \\
\bottomrule
\end{tabular}
\end{table}

В таблице~\ref{tab:landscape-comparison} представлен сравнительный анализ сложности моделей.
Для всех рассмотренных архитектур ландшафтная мера сложности экспоненциально зависит от числа слоев $L$, что является общим свойством всех типов нейронных сетей.
Следовательно, увеличение глубины сети приводит к экспоненциальному росту сложности оптимизационного ландшафта, что требует экспоненциально большего объема данных для стабилизации функции потерь.
Параметры архитектуры, отличные от глубины (ширина слоя, число каналов, размер ядра), входят в оценку полиномиально, что указывает на более умеренное влияние этих параметров на сложность ландшафта по сравнению с глубиной сети.

Полносвязные сети характеризуются тем, что ландшафтная мера зависит только от глубины $L$ и ширины $h$ скрытых слоев, не учитывая размерность входных данных.
Это отражает универсальность полносвязных сетей, способных обрабатывать данные произвольной размерности после их векторизации.
Ландшафтная мера для полносвязных сетей имеет асимптотику, пропорциональную $L(hM)^{2L}$, демонстрируя экспоненциальную зависимость от глубины и полиномиальную зависимость от ширины слоев.

В отличие от полносвязных сетей, 1D-сверточные сети характеризуются тем, что ландшафтная мера включает линейную зависимость от длины входной последовательности $d$.
Ландшафтная мера для 1D-сверточных сетей имеет асимптотику, пропорциональную $L(C^2M^2kd)^{L}$, где $C$~--- максимальное число каналов, $k$~--- размер свертки.
Следовательно, для обработки более длинных последовательностей требуется пропорционально больше данных для стабилизации ландшафта.

Для 2D-сверточных сетей ландшафтная мера включает квадратичную зависимость от размеров входного изображения $m \times n$, что отражает двумерную природу данных.
Ландшафтная мера для 2D-сверточных сетей имеет асимптотику, пропорциональную $C^2k^2L(C^2k^2M^2mn)^{L}$, демонстрируя более быстрый рост сложности по сравнению с 1D-архитектурами при увеличении размеров входных данных.

Сверточные сети с операциями MaxPooling и AvgPooling характеризуются тем, что ландшафтная мера включает дополнительный регуляризирующий множитель $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2}$, который уменьшает сложность оптимизационного ландшафта.
Ландшафтная мера для таких сетей имеет асимптотику, пропорциональную $\left(\frac{1}{k_{\mathrm{pool}}^2}\right)^{L-l+2} C^2k^2L(C^2k^2M^2mn)^{L}$, где $k_{\mathrm{pool}}$~--- размер ядра пулинга, $l$~--- позиция слоя пулинга.
Влияние пулинга зависит от его позиции в сети: чем раньше расположен слой пулинга, тем сильнее его редуцирующее воздействие.
Операции пулинга частично компенсируют экспоненциальный рост сложности с увеличением глубины сети, демонстрируя регуляризирующий эффект.

Полученные оценки позволяют количественно оценить влияние различных гиперпараметров архитектуры на сложность оптимизационного ландшафта и, следовательно, на требования к объему обучающих данных.
Для глубоких сетей (большие значения $L$) экспоненциальный рост сложности означает, что даже небольшое увеличение глубины требует существенного увеличения объема данных.
Напротив, увеличение ширины слоев или числа каналов приводит к полиномиальному росту сложности, что делает эти параметры более управляемыми с точки зрения требований к данным.

Сравнение полносвязных и сверточных архитектур показывает, что сверточные сети более эффективно используют структуру данных (пространственную или временную), что отражается в явной зависимости ландшафтной меры от размеров входных данных.
Это согласуется с практическим опытом, показывающим, что сверточные сети часто требуют меньшего объема данных для достижения заданного качества по сравнению с полносвязными сетями при работе со структурированными данными.
Анализ влияния операций пулинга демонстрирует их регуляризирующую роль в глубоких сверточных сетях: пулинг не только уменьшает вычислительную сложность, но и снижает сложность оптимизационного ландшафта, что способствует более стабильному обучению.
Практическое значение данного результата заключается в возможности целенаправленного размещения слоев пулинга для управления сложностью модели и требованиями к объему обучающих данных.

\section{Заключение}

В работе предложен теоретический аппарат для анализа ландшафтной сложности моделей глубокого обучения на основе спектральных свойств матриц Гессе функции потерь.

Введено понятие условной сложности модели и ландшафтной меры сложности, определяемой через спектральные свойства матриц Гессе и характеризующей изменение кривизны оптимизационного ландшафта при добавлении новых объектов данных.
Показано, что ландшафтная мера сложности позволяет количественно описывать чувствительность оптимизационного ландшафта к расширению выборки и служит инструментом анализа сложности настройки параметров нейросетевых моделей.

На основе теоретических оценок нормы матрицы Гессе, полученных в работах~\cite{grabovoi2024unraveling748584228, grabovoi2024convnets743111032}, установлены асимптотики ландшафтной меры сложности для различных архитектур: для полносвязных сетей (лемма~\ref{chapter:complex:corollary-theorem-kiselev-loss}), для 1D- и 2D-сверточных сетей (леммы~\ref{chapter:complex:corollary-theorem-1Dconv-loss} и~\ref{chapter:complex:corollary-theorem-2Dconv-loss}), а также для сверточных сетей с операциями MaxPooling и AvgPooling (леммы~\ref{chapter:complex:corollary-theorem-maxpool-loss} и~\ref{chapter:complex:corollary-theorem-avgpool-loss}).
Проведен сравнительный анализ ландшафтной меры сложности для различных архитектур, результаты которого представлены в таблице~\ref{tab:landscape-comparison}.

Полученные результаты формируют теоретическую основу для количественного анализа ландшафтной сложности моделей глубокого обучения и могут быть использованы при проектировании архитектур нейронных сетей и выборе режимов их обучения.

\bibliography{main}

\end{document}